import os
import pkgutil
import importlib
import requests
from modules import telegram
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import fake_useragent

_DETECTORS = []

def _load_proxies() -> list[str]:
    p = Path("proxies.txt")
    if not p.exists():
        print("‚ö†Ô∏è  –§–∞–π–ª proxies.txt –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –ø—Ä–æ–∫—Å–∏")
        return []
    print("üìñ  –ó–∞–≥—Ä—É–∂–∞—é —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∫—Å–∏ –∏–∑ proxies.txt")
    lines = [l.strip() for l in p.read_text().splitlines() if l.strip()]
    print(f"üî¢  –ù–∞–π–¥–µ–Ω–æ {len(lines)} —Å—Ç—Ä–æ–∫(–∏) –ø—Ä–æ–∫—Å–∏")
    return lines

def _parse_proxy(line: str) -> dict[str, str] | None:
    try:
        host, port, proto, *rest = line.split(":")
    except ValueError:
        print(f"‚ùå  –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–∫—Å–∏: {line}")
        return None
    creds = ""
    if len(rest) == 2:
        user, pwd = rest
        creds = f"{user}:{pwd}@"
    proxy_url = f"{proto}://{creds}{host}:{port}"
    return {"http": proxy_url, "https": proxy_url}

def add_detectors():
    if _DETECTORS:
        return
    folder = Path(__file__).parent
    package = __package__ or ""
    print("üîç  –ò—â—É –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏", folder)
    for _, mod_name, is_pkg in pkgutil.iter_modules([folder]):
        if is_pkg:
            continue
        full_name = f"{package}.{mod_name}" if package else mod_name
        try:
            mod = importlib.import_module(full_name)
            if hasattr(mod, "detect"):
                _DETECTORS.append(mod.detect)
                print(f"‚úÖ  –î–µ—Ç–µ–∫—Ç–æ—Ä '{full_name}' –∑–∞–≥—Ä—É–∂–µ–Ω")
            else:
                print(f"‚ö†Ô∏è  –ú–æ–¥—É–ª—å '{full_name}' –±–µ–∑ —Ñ—É–Ω–∫—Ü–∏–∏ detect")
        except Exception as e:
            print(f"‚ùå  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ '{full_name}': {e}")

def _collect_js(url: str, s: requests.Session) -> tuple[str, dict]:
    print(f"üåê  GET {url}")
    r = s.get(url, timeout=10)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "lxml")
    
    code = []
    
    for tag in soup.find_all("script"):
        if tag.get("src"):
            src = urljoin(url, tag["src"])
            try:
                print(f"üì•  –°–∫–∞—á–∏–≤–∞—é —Å–∫—Ä–∏–ø—Ç {src}")
                code.append(s.get(src, timeout=7).text)
            except Exception as e:
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {src}: {e}")
        elif tag.string:
            code.append(tag.string)

    for tag in soup.find_all("link", rel="stylesheet"):
        href = tag.get("href")
        if href:
            css_url = urljoin(url, href)
            try:
                print(f"üì•  –°–∫–∞—á–∏–≤–∞—é CSS {css_url}")
                code.append(s.get(css_url, timeout=7).text)
            except Exception as e:
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {css_url}: {e}")

    print(f"üóÇÔ∏è  –°–æ–±—Ä–∞–Ω–æ {len(code)} JS –∏ CSS —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
    return "\n".join(code), r.headers

def run(domain: str, tg_token: str, tg_chat: int) -> None:
    if not domain.startswith(("http://", "https://")):
        domain = "https://" + domain

    ua = None
    if fake_useragent:
        try:
            ua = fake_useragent.UserAgent().random
            print("üï∂Ô∏è  –°–ª—É—á–∞–π–Ω—ã–π User-Agent:", ua)
        except Exception as e:
            print("‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å User-Agent:", e)

    proxies_list = _load_proxies()
    if not proxies_list:
        proxies_list = [None]  

    add_detectors()
    if not _DETECTORS:
        print("‚ùå  –ù–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ ‚Äì –≤—ã—Ö–æ–¥–∏–º")
        return

    for line in proxies_list:
        proxies = _parse_proxy(line) if line else None
        print("üöÄ  –ü—Ä–æ–±—É—é –ø—Ä–æ–∫—Å–∏:", proxies or "–ë–ï–ó –ü–†–û–ö–°–ò")
        with requests.Session() as s:
            if proxies:
                s.proxies = proxies
            if ua:
                s.headers.update({"User-Agent": ua})
            try:
                js, headers = _collect_js(domain, s)
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Å–∞–π—Ç–∞ —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏ {proxies}: {e}")
                continue

            found = False
            reses = []
            for detector in _DETECTORS:
                try:
                    res = detector(js, headers)
                    if res:
                        print(f"üéØ  –î–µ—Ç–µ–∫—Ç–æ—Ä {detector.__module__} –Ω–∞—à—ë–ª: {res}")
                        reses += res + " \n"
                        found = True
                except Exception as e:
                    print(f"‚ö†Ô∏è  –î–µ—Ç–µ–∫—Ç–æ—Ä {detector.__module__} —É–ø–∞–ª: {e}")

            if found:
                print("üèÅ  –ì–æ—Ç–æ–≤–æ, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é —Ü–∏–∫–ª")
                if tg_token and tg_chat:
                    try:
                        print(f"üì° –û—Ç–ø—Ä–∞–≤–ª—è—é –≤ Telegram —á–∞—Ç {tg_chat}")
                        telegram.send_message(tg_token, tg_chat, f"–ù–∞–π–¥–µ–Ω—ã —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ –¥–ª—è —Å–∞–π—Ç–∞: {domain}\n" + "".join(map(str, reses)))
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ç–µ–ª–µ–≥—Ä–∞–º–º: {e}")
                return

    print("üòî  –í—Å–µ –ø—Ä–æ–∫—Å–∏ –Ω–µ—Ä–∞–±–æ—á–∏–µ –∏–ª–∏ –¥–µ—Ç–µ–∫—Ç–æ—Ä—ã –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏")
